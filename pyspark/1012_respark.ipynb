{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from haversine import haversine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# pyspark.sql.maxToStringFields=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[설정](https://steemit.com/cloudera/@uneedcomms/dw-ver-2-spark)  \n",
    "을 분명 아래처럼 하는데 뭔가  \n",
    "그 Executor에 바뀐게 하나도 없음(논란거리)  \n",
    "도대체 저번에 11GB 바뀐게 뭔지가 의문\n",
    "\n",
    "## +++  \n",
    "막 이런 spark tuning 예시들 보면 다들 스펙 장난아니더라  \n",
    "cpu 16코어 메모리 120GB 짜리 4 server로 spark 돌려서(대신 aws인듯)   \n",
    "기본 튜닝하면 되게 크게 설정이 가능한데  \n",
    "소올직히 로컬(나는 찾아보니 cpu 4코어 메모리 16GB)에서는 튜닝해도 별 감흥 없음...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('yarn.nodemanager.resource.memory-mb', '14g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('yarn.nodemanager.resource.cpu-vcores', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('yarn.scheduler.maximum-allocation-mb', '14g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('yarn.scheduler.minimum-allocation-mb', '1g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.cores', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.instances', '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '3g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"App Name\")\n",
    "sql2 = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = range(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_da = sc.parallelize(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[1] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Crime Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"./Crimes_Chicago.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RDD로 csv 파일 열기](https://stackoverflow.com/questions/43239969/read-csv-file-with-strings-to-rdd-spark)  \n",
    "[RDD 관련 정보1](https://bcho.tistory.com/1027?category=563141)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data = spark.read.csv(\"./Crimes_Chicago.csv\", header=True).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_data.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DropNaindata(data):\n",
    "    return len(data) != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "usedef = rdd_data.filter(DropNaindata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오 rddfile.count() 하면 count 나와야하는데 엄청 오래 걸림...^^;  \n",
    "rdd도 답이 아닌가..  \n",
    "Google cloud 걔 써야하나 싶고 그렇다...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago = data.alias(\"chicago\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago = data.na.drop(subset =[\"Longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago.createOrReplaceTempView(\"CHICAGO\")\n",
    "year_count = spark.sql(\"SELECT Year, count(Year) AS Count FROM CHICAGO GROUP BY Year ORDER BY Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복사\n",
    "chicago_md = chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_md = chicago_md.withColumn('Month', chicago_md['Date'].substr(1,2))\n",
    "chicago_md = chicago_md.withColumn('Day', chicago_md['Date'].substr(4,2))\n",
    "chicago_md = chicago_md.withColumn('Time', chicago_md['Date'].substr(12,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_md_rename = chicago_md.select('ID',col(\"Case Number\").alias(\"Case_Number\"), 'Date','Block','IUCR',\n",
    "                                      col(\"Primary Type\").alias(\"Primary_Type\"),'Description',\n",
    "                                      col(\"Location Description\").alias(\"Location_Description\"), \n",
    "                                      'Arrest','Domestic','Beat','District','Ward',\n",
    "                                      col(\"Community Area\").alias(\"Community_Area\"),\n",
    "                                      col(\"FBI Code\").alias(\"FBI_Code\"),\n",
    "                                     col(\"X Coordinate\").alias(\"X_Coordinate\"),col(\"Y Coordinate\").alias(\"Y_Coordinate\"),\n",
    "                                      col(\"Updated On\").alias(\"Updated_On\"),'Latitude','Longitude','Location',\n",
    "                                     col(\"Historical Wards 2003-2015\").alias(\"Historical_Wards_20032015\"),\n",
    "                                     col(\"Zip Codes\").alias(\"Zip_Codes\"),col(\"Community Areas\").alias(\"Community_Areas\"),\n",
    "                                     col(\"Census Tracts\").alias(\"Census_Tracts\"), 'Wards',\n",
    "                                      col(\"Boundaries - ZIP Codes\").alias(\"Boundaries_ZIPCodes\"),\n",
    "                                     col(\"Police Districts\").alias(\"Police_Districts\"), col(\"Police Beats\").alias(\"Police_Beats\"),\n",
    "                                      'Year','Month', 'Day','Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_md_rename.createOrReplaceTempView(\"CHICAGO_MD_RE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 년도별 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_lst = list(year_count.select('Year').toPandas()['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlst = []\n",
    "for i in range(len(year_lst)):\n",
    "#     print(year_lst[i])\n",
    "    tmp = spark.sql(\"SELECT * FROM CHICAGO_MD_RE WHERE Year = {}\".format(year_lst[i]))\n",
    "    yearlst.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime1617 = yearlst[15].union(yearlst[16])\n",
    "crime1718 = yearlst[16].union(yearlst[17])\n",
    "crime1819 = yearlst[17].union(yearlst[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open School Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school = spark.read.csv(\"./school_profile.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_loc = school.select(\"School_ID\",\"School_Latitude\",\"School_Longitude\",\"SY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * School Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_income = school.select('School_ID','Student_Count_Total','Student_Count_Low_Income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_income = school_income.withColumn(\"Low_Income_Ratio\", \n",
    "                         (col(\"Student_Count_Low_Income\")/col(\"Student_Count_Total\")))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# school_income.sort(col(\"Low_Income_Ratio\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_income_ratio = school_income.select(\"School_ID\",\"Low_Income_Ratio\").sort(col(\"Low_Income_Ratio\").desc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School time join School loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key 중복 없이 JOIN 하는 방법\n",
    "\n",
    "join_school_loc_ratio = school_loc.join(school_income_ratio, [\"School_ID\"], how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SY 기준 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sy_lst = join_school_loc_ratio.select(\"SY\").distinct().toPandas()[\"SY\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_yearlst_time = []\n",
    "for i in range(len(t_sy_lst)):\n",
    "#     print(t_sy_lst[i])\n",
    "    tmp = join_school_loc_ratio.select(col(\"*\")).where(col(\"SY\") == t_sy_lst[i])\n",
    "    sc_yearlst_time.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sy_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Location & School Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EX) 1617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime1617_df = crime1617.select('Primary_Type', 'Latitude', 'Longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_join = sc_yearlst_time[0].crossJoin(crime1617_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_join.createOrReplaceTempView(\"DISTANCE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_table = spark.sql(\"SELECT *, ROUND((6371*acos(cos(radians(School_Latitude))*cos(radians(Latitude))*cos(radians(Longitude)-radians(School_Longitude))+sin(radians(School_Latitude))*sin(radians(Latitude)))),6) AS Distance FROM DISTANCE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_group1_1 = distance_table.filter(distance_table.Distance <=2).groupby(\"School_ID\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distance_group1_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
