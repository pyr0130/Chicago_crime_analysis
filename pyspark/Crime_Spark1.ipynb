{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext, Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data 파일 열기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* header를 같이 불러오려면 header=True를 반드시 써줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"./Crimes_Chicago.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결측지 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago = data.alias(\"chicago\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 특정 열에 결측지 있는 행 제거하는 구문\n",
    "\n",
    "(참조) https://stackoverflow.com/questions/35477472/difference-between-na-drop-and-filtercol-isnotnull-apache-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago = data.na.drop(subset =[\"Longitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 행 개수 count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chicago.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 임시저장..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이 부분에서 에러 발생했는데 pyspark 버전 문제 때문이었음\n",
    "\n",
    "\n",
    "cmd에 pip install pyspark==2.3.0\n",
    "\n",
    "(참조) https://stackoverflow.com/questions/54546513/py4jjavaerror-an-error-occurred-while-calling\n",
    "\n",
    "이라고 생각했지만 놀랍게도 메모리 터져서 안됨\n",
    "왜 터지고 난리죠..?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_chicago = chicago.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 년도별 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chicago.filter('Year==2001').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dd = SQLContext.sql(\"SELECT * from chicago WHERE Year <> '2011'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* view 만들어서 sql문 사용 가능하도록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago.createOrReplaceTempView(\"CHICAGO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"SELECT * FROM CHICAGO WHERE Year = '2001'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chicago.filter(chicago[\"Year\"] == '2001').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 년도별 카운트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count = spark.sql(\"SELECT Year, count(Year) AS Count FROM CHICAGO GROUP BY Year ORDER BY Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(year_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 찾아보니 Plot 쓰려면 pyspark.sql.dataframe인 상태로는 못하고 로컬로 가져와야 해서\n",
    "\n",
    "collect()\n",
    "\n",
    "toPandas()\n",
    "\n",
    "\n",
    "사용해야 한다고 함\n",
    "\n",
    "근데 또 너무 큰거 로컬로 가져오려고 하면 메모리가 터져버리니 아이러니.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_year_count = year_count.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6), dpi=80)\n",
    "plt.xlabel('Year', size = 15)\n",
    "plt.ylabel('Count',  size = 15)\n",
    "plt.title('Chicago Data Count by Year', size = 17)\n",
    "plt.bar(pd_year_count['Year'],pd_year_count['Count'], color = \"#ff6f61\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 년도별 데이터 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pySpark dataframe column to list in python (1)\n",
    "\n",
    "아래 방법 쓰려고 했는데 ROW자체가 요소로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyear_lst = year_count.select('Year').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(year_lst[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pySpark dataframe column to list in python (2)\n",
    "\n",
    "이 방법은 pandas dataframe -> list로 바꾸는 방식임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_lst = list(year_count.select('Year').toPandas()['Year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* sql문에서 python 변수 사용하는 방법\n",
    "\n",
    "(참조) https://stackoverflow.com/questions/44582450/how-to-pass-variables-in-spark-sql-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=5\n",
    "tmp = spark.sql(\"SELECT * FROM CHICAGO WHERE Year = {}\".format(year_lst[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* toPandas 넣는 순간 억겁의 시간을 기다리게 되어서 멈춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yearlst = []\n",
    "# for i in range(len(year_lst)):\n",
    "#     print(year_lst[i])\n",
    "#     tmp = spark.sql(\"SELECT * FROM CHICAGO WHERE Year = {}\".format(year_lst[i]))\n",
    "#     print(i,\" finish\")\n",
    "#     yearlst.append(tmp.toPandas())\n",
    "#     print(\"*** to pandas ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그래서 우선 Pyspark 형태로 list 저장함\n",
    "\n",
    "차라리 이걸 각각 csv로 저장한다음에 pandas dataframe으로 csv 읽어서 plot 그리거나\n",
    "\n",
    "계산할거면 pyspark 쓴 후에 csv 저장하고..이런식으로 하는게 더 빠를 수도 \n",
    "\n",
    "(년도 한두개만 toPandas는 기다릴만 한듯)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlst = []\n",
    "for i in range(len(year_lst)):\n",
    "#     print(year_lst[i])\n",
    "    tmp = spark.sql(\"SELECT * FROM CHICAGO WHERE Year = {}\".format(year_lst[i]))\n",
    "    yearlst.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlst[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아니면 SQL에서 바로 계산해버리기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 월별 평균치 통계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 를 위해서 Month, Day 열을 추가해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = spark.sql(\"SELECT * FROM CHICAGO )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = spark.sql(\"SELECT *, SUBSTR(Date,0,2) AS Month, SUV AS Day FROM CHICAGO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
